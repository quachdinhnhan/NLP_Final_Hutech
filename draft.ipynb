{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "743617d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c60502f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file from folder\n",
    "\n",
    "file_path = \"DUC_TEXT/test/d112h\"  \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    doc_file = file.read()\n",
    "\n",
    "#print(doc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0ddad",
   "metadata": {},
   "source": [
    "# parsing docs to doc_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a69b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(docs_string):\n",
    "    doc_list = []\n",
    "    # Find all sentence tags\n",
    "    sentences = re.findall(r'<s.*?</s>', docs_string, re.DOTALL)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        doc_info = {}\n",
    "        # Extract docid, num, and wdcount from the tag\n",
    "        match = re.search(r'<s docid=\"(.*?)\" num=\"(.*?)\" wdcount=\"(.*?)\">', sentence)\n",
    "        if match:\n",
    "            doc_info['docid'] = match.group(1)\n",
    "            doc_info['num'] = int(match.group(2))\n",
    "            doc_info['wdcount'] = int(match.group(3))\n",
    "\n",
    "            # Extract the text content within the tag\n",
    "            text = re.sub(r'<s.*?\">', '', sentence)\n",
    "            text = re.sub(r'</s>', '', text).strip()\n",
    "            doc_info['full_text'] = text\n",
    "\n",
    "            doc_list.append(doc_info)\n",
    "\n",
    "    return doc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f10b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of dictionaries\n",
    "docs_data = parse_docs(doc_file)\n",
    "\n",
    "# Print the resulting list of dictionaries\n",
    "#print(docs_data)\n",
    "\n",
    "# If you want a dictionary where keys are docid-num combinations\n",
    "docs_dict = {}\n",
    "for doc in docs_data:\n",
    "    key = f\"{doc['num']}\"\n",
    "    docs_dict[key] = {\n",
    "        'docid': doc['docid'],\n",
    "\n",
    "        'wdcount': doc['wdcount'],\n",
    "        'full_text': doc['full_text']\n",
    "    }\n",
    "\n",
    "# Print the resulting dictionary\n",
    "#print(docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f9e34",
   "metadata": {},
   "source": [
    "compare full text to each other, if they have n (for ex. 4, can change the number later for testing) common words then say they have an connection, save the index of the sentence and the connection btw two sentences as boolean  to an balanced two-dimensional array,\n",
    " \"\"\"\n",
    "    Checks if two strings have a connection based on common words.\n",
    "\n",
    "    Args:\n",
    "        text1 (str): The first string.\n",
    "        text2 (str): The second string.\n",
    "        min_common_words (int): The minimum number of common words required for a connection.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the strings have a connection, False otherwise.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d5ba1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_connection(text1, text2, min_common_words=7):\n",
    "   \n",
    "    words1 = set(re.findall(r'\\b\\w+\\b', text1.lower()))\n",
    "    words2 = set(re.findall(r'\\b\\w+\\b', text2.lower()))\n",
    "    common_words = words1.intersection(words2)\n",
    "    return len(common_words) >= min_common_words\n",
    "\n",
    "# Get the number of sentences\n",
    "num_sentences = len(docs_dict)\n",
    "\n",
    "# Initialize a balanced two-dimensional array (matrix) with False\n",
    "connection_matrix = np.full((num_sentences, num_sentences), False, dtype=bool)\n",
    "\n",
    "# Iterate through all pairs of sentences to check for connections\n",
    "sentence_keys = list(docs_dict.keys())\n",
    "for i in range(num_sentences):\n",
    "    for j in range(num_sentences):\n",
    "        if i != j:  # Avoid comparing a sentence to itself\n",
    "            text1 = docs_dict[sentence_keys[i]]['full_text']\n",
    "            text2 = docs_dict[sentence_keys[j]]['full_text']\n",
    "            if has_connection(text1, text2):\n",
    "                connection_matrix[i, j] = True\n",
    "\n",
    "# Print the connection matrix\n",
    "# print(\"\\nConnection Matrix:\")\n",
    "# print(connection_matrix)\n",
    "\n",
    "# You can also print which sentences are connected\n",
    "# print(\"\\nSentence Connections:\")\n",
    "# for i in range(num_sentences):\n",
    "#     for j in range(num_sentences):\n",
    "#         if connection_matrix[i, j]:\n",
    "#             print(f\"Sentence {sentence_keys[i]} is connected to Sentence {sentence_keys[j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff659d5",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# PageRank Formula\n",
    "\n",
    "# The PageRank formula is defined as:\n",
    "\n",
    "$$\n",
    "PR(i) = \\frac{1 - d}{N} + d \\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $PR(i)$ is the PageRank of node $i$\n",
    "- $d$ is the damping factor (usually 0.85)\n",
    "- $N$ is the total number of nodes\n",
    "- $M(i)$ are nodes linking to $i$\n",
    "- $L(j)$ is the number of outgoing links from node $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c29e079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank did not converge after 5 iterations.\n"
     ]
    }
   ],
   "source": [
    "def pagerank(matrix, damping_factor=0.85, max_iterations=5, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Calculates the PageRank score for each node in a graph.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.array): The adjacency matrix of the graph (boolean or integer).\n",
    "                           matrix[i, j] is True if there is a link from node i to node j.\n",
    "        damping_factor (float): The damping factor (usually 0.85).\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "        tolerance (float): Convergence tolerance.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A numpy array containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    num_nodes = matrix.shape[0]\n",
    "\n",
    "    # Convert boolean matrix to float and normalize rows\n",
    "    # Create the transition matrix\n",
    "    transition_matrix = matrix.astype(float)\n",
    "    row_sums = transition_matrix.sum(axis=1)\n",
    "    # Handle nodes with no outgoing links (dangling nodes)\n",
    "    # Add links to all other nodes, distributing their weight evenly\n",
    "    dangling_nodes = np.where(row_sums == 0)[0]\n",
    "    for node in dangling_nodes:\n",
    "        transition_matrix[node, :] = 1.0 / num_nodes\n",
    "    # Normalize non-dangling rows\n",
    "    non_dangling_rows = np.where(row_sums != 0)[0]\n",
    "    transition_matrix[non_dangling_rows, :] /= row_sums[non_dangling_rows, None]\n",
    "\n",
    "    # Initialize PageRank vector\n",
    "    pr = np.ones(num_nodes) / num_nodes\n",
    "\n",
    "    # Iteratively calculate PageRank\n",
    "    for iteration in range(max_iterations):\n",
    "        prev_pr = pr.copy()\n",
    "        # PageRank formula: PR(i) = (1-d)/N + d * sum(PR(j)/L(j)) for all j linking to i\n",
    "        # In matrix form: PR = (1-d)/N * 1 + d * M.T @ PR  where M is the transition matrix\n",
    "        pr = (1 - damping_factor) / num_nodes + damping_factor * (transition_matrix.T @ pr)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(pr - prev_pr, ord=1) < tolerance:\n",
    "            print(f\"PageRank converged after {iteration + 1} iterations.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"PageRank did not converge after {max_iterations} iterations.\")\n",
    "\n",
    "    return pr\n",
    "\n",
    "# Calculate PageRank for the sentences using the connection matrix\n",
    "sentence_pagerank_scores = pagerank(connection_matrix)\n",
    "\n",
    "# Print the PageRank scores\n",
    "# print(\"\\nPageRank Scores for Sentences:\")\n",
    "# sentence_keys = list(docs_dict.keys())\n",
    "# for i, score in enumerate(sentence_pagerank_scores):\n",
    "#     print(f\"Sentence {sentence_keys[i]}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d36ad7",
   "metadata": {},
   "source": [
    "sort the score decendant and print out 10 of highest score snetences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentence keys and scores into a list of tuples\n",
    "sentence_scores = list(zip(sentence_keys, sentence_pagerank_scores))\n",
    "\n",
    "# Sort the sentences based on their PageRank scores in descending order\n",
    "sorted_sentences = sorted(sentence_scores, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Print the 10 highest scoring sentences\n",
    "print(\"\\nTop 10 Highest Scoring Sentences (by PageRank):\")\n",
    "for i, (key, score) in enumerate(sorted_sentences[:10]):\n",
    "    print(f\"{i+1}. Sentence {key} (Score: {score:.6f}):\")\n",
    "    print(docs_dict[key]['full_text'])\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75587c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 sentences written to output/d112h_top_sentences.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the input file name without the extension\n",
    "input_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_dir, f'{input_filename}_top_sentences.txt')\n",
    "\n",
    "# Write the top sentences to the output file in the desired format\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for key, score in sorted_sentences[:10]:\n",
    "        docid = docs_dict[key]['docid']\n",
    "        wdcount = docs_dict[key]['wdcount']\n",
    "        full_text = docs_dict[key]['full_text']\n",
    "        # Reconstruct the original sentence tag format\n",
    "        outfile.write(f'<s docid=\"{docid}\" num=\"{key}\" wdcount=\"{wdcount}\"> {full_text}</s>\\n')\n",
    "\n",
    "print(f\"\\nTop 10 sentences written to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
