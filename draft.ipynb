{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "743617d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c60502f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file from folder\n",
    "\n",
    "file_path = \"DUC_TEXT/test/d112h\"  \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    doc_file = file.read()\n",
    "\n",
    "#print(doc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0ddad",
   "metadata": {},
   "source": [
    "# parsing docs to doc_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02a69b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(docs_string):\n",
    "    doc_list = []\n",
    "    # Find all sentence tags\n",
    "    sentences = re.findall(r'<s.*?</s>', docs_string, re.DOTALL)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        doc_info = {}\n",
    "        # Extract docid, num, and wdcount from the tag\n",
    "        match = re.search(r'<s docid=\"(.*?)\" num=\"(.*?)\" wdcount=\"(.*?)\">', sentence)\n",
    "        if match:\n",
    "            doc_info['docid'] = match.group(1)\n",
    "            doc_info['num'] = int(match.group(2))\n",
    "            doc_info['wdcount'] = int(match.group(3))\n",
    "\n",
    "            # Extract the text content within the tag\n",
    "            text = re.sub(r'<s.*?\">', '', sentence)\n",
    "            text = re.sub(r'</s>', '', text).strip()\n",
    "            doc_info['full_text'] = text\n",
    "\n",
    "            doc_list.append(doc_info)\n",
    "\n",
    "    return doc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74f10b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of dictionaries\n",
    "docs_data = parse_docs(doc_file)\n",
    "\n",
    "# Print the resulting list of dictionaries\n",
    "#print(docs_data)\n",
    "\n",
    "# If you want a dictionary where keys are docid-num combinations\n",
    "docs_dict = {}\n",
    "for doc in docs_data:\n",
    "    key = f\"{doc['num']}\"\n",
    "    docs_dict[key] = {\n",
    "        'docid': doc['docid'],\n",
    "\n",
    "        'wdcount': doc['wdcount'],\n",
    "        'full_text': doc['full_text']\n",
    "    }\n",
    "\n",
    "# Print the resulting dictionary\n",
    "#print(docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f9e34",
   "metadata": {},
   "source": [
    "compare full text to each other, if they have n (for ex. 4, can change the number later for testing) common words then say they have an connection, save the index of the sentence and the connection btw two sentences as boolean  to an balanced two-dimensional array,\n",
    " \"\"\"\n",
    "    Checks if two strings have a connection based on common words.\n",
    "\n",
    "    Args:\n",
    "        text1 (str): The first string.\n",
    "        text2 (str): The second string.\n",
    "        min_common_words (int): The minimum number of common words required for a connection.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the strings have a connection, False otherwise.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d5ba1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_connection(text1, text2, min_common_words=7):\n",
    "   \n",
    "    words1 = set(re.findall(r'\\b\\w+\\b', text1.lower()))\n",
    "    words2 = set(re.findall(r'\\b\\w+\\b', text2.lower()))\n",
    "    common_words = words1.intersection(words2)\n",
    "    return len(common_words) >= min_common_words\n",
    "\n",
    "# Get the number of sentences\n",
    "num_sentences = len(docs_dict)\n",
    "\n",
    "# Initialize a balanced two-dimensional array (matrix) with False\n",
    "connection_matrix = np.full((num_sentences, num_sentences), False, dtype=bool)\n",
    "\n",
    "# Iterate through all pairs of sentences to check for connections\n",
    "sentence_keys = list(docs_dict.keys())\n",
    "for i in range(num_sentences):\n",
    "    for j in range(num_sentences):\n",
    "        if i != j:  # Avoid comparing a sentence to itself\n",
    "            text1 = docs_dict[sentence_keys[i]]['full_text']\n",
    "            text2 = docs_dict[sentence_keys[j]]['full_text']\n",
    "            if has_connection(text1, text2):\n",
    "                connection_matrix[i, j] = True\n",
    "\n",
    "# Print the connection matrix\n",
    "# print(\"\\nConnection Matrix:\")\n",
    "# print(connection_matrix)\n",
    "\n",
    "# You can also print which sentences are connected\n",
    "# print(\"\\nSentence Connections:\")\n",
    "# for i in range(num_sentences):\n",
    "#     for j in range(num_sentences):\n",
    "#         if connection_matrix[i, j]:\n",
    "#             print(f\"Sentence {sentence_keys[i]} is connected to Sentence {sentence_keys[j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff659d5",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# PageRank Formula\n",
    "\n",
    "# The PageRank formula is defined as:\n",
    "\n",
    "$$\n",
    "PR(i) = \\frac{1 - d}{N} + d \\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $PR(i)$ is the PageRank of node $i$\n",
    "- $d$ is the damping factor (usually 0.85)\n",
    "- $N$ is the total number of nodes\n",
    "- $M(i)$ are nodes linking to $i$\n",
    "- $L(j)$ is the number of outgoing links from node $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c29e079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank did not converge after 5 iterations.\n"
     ]
    }
   ],
   "source": [
    "def pagerank(matrix, damping_factor=0.85, max_iterations=5, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Calculates the PageRank score for each node in a graph.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.array): The adjacency matrix of the graph (boolean or integer).\n",
    "                           matrix[i, j] is True if there is a link from node i to node j.\n",
    "        damping_factor (float): The damping factor (usually 0.85).\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "        tolerance (float): Convergence tolerance.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A numpy array containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    num_nodes = matrix.shape[0]\n",
    "\n",
    "    # Convert boolean matrix to float and normalize rows\n",
    "    # Create the transition matrix\n",
    "    transition_matrix = matrix.astype(float)\n",
    "    row_sums = transition_matrix.sum(axis=1)\n",
    "    # Handle nodes with no outgoing links (dangling nodes)\n",
    "    # Add links to all other nodes, distributing their weight evenly\n",
    "    dangling_nodes = np.where(row_sums == 0)[0]\n",
    "    for node in dangling_nodes:\n",
    "        transition_matrix[node, :] = 1.0 / num_nodes\n",
    "    # Normalize non-dangling rows\n",
    "    non_dangling_rows = np.where(row_sums != 0)[0]\n",
    "    transition_matrix[non_dangling_rows, :] /= row_sums[non_dangling_rows, None]\n",
    "\n",
    "    # Initialize PageRank vector\n",
    "    pr = np.ones(num_nodes) / num_nodes\n",
    "\n",
    "    # Iteratively calculate PageRank\n",
    "    for iteration in range(max_iterations):\n",
    "        prev_pr = pr.copy()\n",
    "        # PageRank formula: PR(i) = (1-d)/N + d * sum(PR(j)/L(j)) for all j linking to i\n",
    "        # In matrix form: PR = (1-d)/N * 1 + d * M.T @ PR  where M is the transition matrix\n",
    "        pr = (1 - damping_factor) / num_nodes + damping_factor * (transition_matrix.T @ pr)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(pr - prev_pr, ord=1) < tolerance:\n",
    "            print(f\"PageRank converged after {iteration + 1} iterations.\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"PageRank did not converge after {max_iterations} iterations.\")\n",
    "\n",
    "    return pr\n",
    "\n",
    "# Calculate PageRank for the sentences using the connection matrix\n",
    "sentence_pagerank_scores = pagerank(connection_matrix)\n",
    "\n",
    "# Print the PageRank scores\n",
    "# print(\"\\nPageRank Scores for Sentences:\")\n",
    "# sentence_keys = list(docs_dict.keys())\n",
    "# for i, score in enumerate(sentence_pagerank_scores):\n",
    "#     print(f\"Sentence {sentence_keys[i]}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d36ad7",
   "metadata": {},
   "source": [
    "sort the score decendant and print out 10 of highest score snetences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f11764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Highest Scoring Sentences (by PageRank):\n",
      "1. Sentence 25 (Score: 0.076781):\n",
      "About #210 million will be used to pay back bank loans and the rest to pay in full a loan from Robert Maxwell Holdings Ltd. Salomon Bros.\n",
      "--------------------\n",
      "2. Sentence 7 (Score: 0.076781):\n",
      "The writs were filed late last week in the State Supreme Court in Manhattan by trustees for Mirror Group Newspapers Pension Scheme and Maxwell Communication Corpora-tion Works Pension Scheme.\n",
      "--------------------\n",
      "3. Sentence 6 (Score: 0.071483):\n",
      "Two writs have been filed alleging that Goldman Sachs, the US-based investment bank, assisted in diverting Pounds 55m from two pension schemes controlled by Robert Maxwell to ensure its own debts from Maxwell interests would be repaid.\n",
      "--------------------\n",
      "4. Sentence 10 (Score: 0.061709):\n",
      "In an interview published today by the Daily Mirror, Robert Maxwell's London flagship newspaper, Kevin Maxwell was quoted as saying of the pensioners:; \"I am desperately sorry for them.\n",
      "--------------------\n",
      "5. Sentence 14 (Score: 0.061709):\n",
      "Kevin Maxwell spoke with the Daily Mirror as he flew to New York on Tuesday for talks on the future of the Daily News, which his father bought in March.\n",
      "--------------------\n",
      "6. Sentence 26 (Score: 0.061709):\n",
      "International is underwriting the international part of the offering, covering approximately one-third of the shares on sale, to be privately placed with institutional investors in the U.S., Canada and Europe.\n",
      "--------------------\n",
      "7. Sentence 27 (Score: 0.061709):\n",
      "The other two-thirds of the shares are to be divided between private and institutional investors in Britain.\n",
      "--------------------\n",
      "8. Sentence 12 (Score: 0.041750):\n",
      "The debts are so enormous that when everything is sorted out there will be nothing left.\"; One week ago, the Serious Fraud Office, a government prosecution agency, began investigating Mirror Group Newspapers PLC's pension fund assets and related matters.\n",
      "--------------------\n",
      "9. Sentence 24 (Score: 0.041750):\n",
      "Proceeds from the share flotation will be used to strengthen and refinance the group's balance sheet.\n",
      "--------------------\n",
      "10. Sentence 8 (Score: 0.012017):\n",
      "Investigators have said Maxwell siphoned money from his companies in a desperate attempt to deal with his empire's crushing debts.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Combine sentence keys and scores into a list of tuples\n",
    "sentence_scores = list(zip(sentence_keys, sentence_pagerank_scores))\n",
    "\n",
    "# Sort the sentences based on their PageRank scores in descending order\n",
    "sorted_sentences = sorted(sentence_scores, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Print the 10 highest scoring sentences\n",
    "print(\"\\nTop 10 Highest Scoring Sentences (by PageRank):\")\n",
    "for i, (key, score) in enumerate(sorted_sentences[:10]):\n",
    "    print(f\"{i+1}. Sentence {key} (Score: {score:.6f}):\")\n",
    "    print(docs_dict[key]['full_text'])\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75587c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 sentences written to output/d112h_top_sentences.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the input file name without the extension\n",
    "input_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_dir, f'{input_filename}_top_sentences.txt')\n",
    "\n",
    "# Write the top sentences to the output file in the desired format\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for key, score in sorted_sentences[:10]:\n",
    "        docid = docs_dict[key]['docid']\n",
    "        wdcount = docs_dict[key]['wdcount']\n",
    "        full_text = docs_dict[key]['full_text']\n",
    "        # Reconstruct the original sentence tag format\n",
    "        outfile.write(f'<s docid=\"{docid}\" num=\"{key}\" wdcount=\"{wdcount}\"> {full_text}</s>\\n')\n",
    "\n",
    "print(f\"\\nTop 10 sentences written to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
