{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ac051271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de5e80",
   "metadata": {},
   "source": [
    "Read file from DUC_TEXT/test\n",
    "Create a dictionary save all the metadata of sentences\n",
    "Create unique sentence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "fe960101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file from DUC_TEXT/test\n",
    "\n",
    "file_path = \"DUC_TEXT/test/d113h\"  \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    doc_file = file.read()\n",
    "\n",
    "# print(\"Read file from DUC_TEXT/test\")\n",
    "# print(doc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "65296f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary save all the metadata of sentences\n",
    "def parse_doc (doc_file):\n",
    "    \"\"\"\n",
    "    Parse the document file and extract sentences metadata.\n",
    "    \n",
    "    Args:\n",
    "        doc_file (str): The content of the document file.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with sentence_id as keys and metadata as values.\n",
    "    \"\"\"\n",
    "    sentences_dict = {}\n",
    "    # Create unique sentence_id\n",
    "    # Initialize sentence_id to 0\n",
    "    sentence_id = 0\n",
    "\n",
    "    # Find all sentences tags and their content in the document\n",
    "    sentence_matches = re.findall(r'<s\\s+docid=\"([^\"]+)\"\\s+num=\"([^\"]+)\"\\s+wdcount=\"([^\"]+)\">\\s*(.*?)\\s*</s>', doc_file, re.DOTALL)\n",
    "\n",
    "    for doc_id, num, wdcount, sentence_text in sentence_matches:\n",
    "        sentences_dict[sentence_id] = {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"num\": num,\n",
    "            \"wdcount\": int(wdcount),\n",
    "            \"sentence_text\": sentence_text.strip()\n",
    "        }\n",
    "        sentence_id += 1    \n",
    "    return sentences_dict\n",
    "\n",
    "sentences_dict = parse_doc(doc_file)\n",
    "# Print the sentences dictionary\n",
    "# print(sentences_dict)\n",
    "# for sid, metadata in sentences_dict.items():\n",
    "#     print(f\"Sentence ID: {sid}, Metadata: {metadata}\")\n",
    "# print(f\"Total sentences processed: {len(sentences_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d326d",
   "metadata": {},
   "source": [
    "Calculate connection by n common word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5164b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_connection(sentence1, sentence2, min_common_words=7):\n",
    "    \"\"\"\n",
    "    Check if two sentences have a connection based on common words.\n",
    "    \n",
    "    Args:\n",
    "        sentence1 (str): The first sentence.\n",
    "        sentence2 (str): The second sentence.\n",
    "        mincommon_words (int): Minimum number of common words to consider a connection.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if there is a connection, False otherwise.\n",
    "    \"\"\"\n",
    "    words1 = set(re.findall(r'\\b\\w+\\b', sentence1.lower()))\n",
    "    words2 = set(re.findall(r'\\b\\w+\\b', sentence2.lower()))\n",
    "    common_words = words1.intersection(words2)\n",
    "    return len(common_words) >= min_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616373b",
   "metadata": {},
   "source": [
    "Create connection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e4b2e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection_matrix(sentences_dict):\n",
    "    \"\"\"\n",
    "    Create a connection matrix based on the sentences metadata.\n",
    "    \n",
    "    Args:\n",
    "        sentences_dict (dict): A dictionary with sentence_id as keys and metadata as values.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A balance two-dimentional matrix where matrix[i][j] is True\n",
    "                    if sentence i and sentence j have connection, and 0 otherwise.\n",
    "    \"\"\"\n",
    "    num_sentences = len(sentences_dict)\n",
    "    connection_matrix = np.full((num_sentences, num_sentences), False, dtype=bool)\n",
    "\n",
    "    # Get all sentence texts\n",
    "    sentence_texts = [sentences_dict[i]['sentence_text'] for i in range(num_sentences)]\n",
    "\n",
    "    # Compare each pair of sentences\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            if has_connection(sentence_texts[i], sentence_texts[j]):\n",
    "                connection_matrix[i][j] = True\n",
    "                connection_matrix[j][i] = True\n",
    "\n",
    "\n",
    "    return connection_matrix\n",
    "connection_matrix = create_connection_matrix(sentences_dict)\n",
    "# Print the connection matrix\n",
    "# print(\"Connection Matrix:\")\n",
    "# print(connection_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6f0bc",
   "metadata": {},
   "source": [
    "Calculate pageRank score\n",
    "The PageRank formula is defined as:\n",
    "\n",
    "$$\n",
    "PR(i) = \\frac{1 - d}{N} + d \\sum_{j \\in M(i)} \\frac{PR(j)}{L(j)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $PR(i)$ is the PageRank of node $i$\n",
    "- $d$ is the damping factor (usually 0.85)\n",
    "- $N$ is the total number of nodes\n",
    "- $M(i)$ are nodes linking to $i$\n",
    "- $L(j)$ is the number of outgoing links from node $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "04d5f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pagerank(connection_matrix, d=0.85, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate PageRank scores based on the connection matrix.\n",
    "    \n",
    "    Args:\n",
    "        connection_matrix (np.ndarray): The connection matrix.\n",
    "        d (float): Damping factor.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "                    This stops the algorithm if it doesnt converge quickly to avoid infinite loops.\n",
    "        tol (float): Tolerance for convergence. \n",
    "                    If the change in PageRank scores between iterations is less than this value, \n",
    "                    the algorithm stops early (it's considered converged)\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: PageRank scores for each sentence.\n",
    "    \"\"\"\n",
    "    # Find the number of nodes in the connection matrix by getting the number of rows of adjacency matrix\n",
    "    num_nodes = connection_matrix.shape[0]\n",
    "    # Convert matrix elements to float\n",
    "    transition_matrix = connection_matrix.astype(float) \n",
    "    # Sum up each row to get the total number of connections for each sentence\n",
    "    row_sums = transition_matrix.sum(axis=1)\n",
    "\n",
    "    # Normalize the transition matrix by dividing each row by its sum\n",
    "    # This ensures that the sum of probabilities in each row equals 1\n",
    "    for i in range(num_nodes):\n",
    "        if row_sums[i] > 0:\n",
    "            transition_matrix[i] /= row_sums[i]\n",
    "        else:\n",
    "            # Handle dangling nodes (nodes with no outgoing edges)\n",
    "            # Assign equal probability to all nodes\n",
    "            transition_matrix[i] = np.ones(num_nodes) / num_nodes\n",
    "    # Initialize PageRank scores to 1/n for each node\n",
    "    pagerank_scores = np.ones(num_nodes) / num_nodes\n",
    "    # Iterate to update PageRank scores\n",
    "    for _ in range(max_iter):\n",
    "        new_pagerank_scores = np.zeros(num_nodes)\n",
    "        for i in range(num_nodes):\n",
    "            # Calculate the new PageRank score for each node\n",
    "            new_pagerank_scores[i] = (1 - d) / num_nodes + d * np.sum(transition_matrix[:, i] * pagerank_scores)\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_pagerank_scores - pagerank_scores, ord=1) < tol:\n",
    "            break\n",
    "        pagerank_scores = new_pagerank_scores\n",
    "    return pagerank_scores\n",
    "pagerank_scores = calculate_pagerank(connection_matrix)\n",
    "# Print the PageRank scores\n",
    "# print(\"PageRank Scores:\")\n",
    "# for i, score in enumerate(pagerank_scores):\n",
    "#     print(f\"Sentence ID {i}: {score:.4f} - {sentences_dict[i]['sentence_text']}\")    \n",
    "    \n",
    "                                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c64a16",
   "metadata": {},
   "source": [
    "Get the 10% highest score sentences to create a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7695c73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Sentences:\n",
      "Sentence ID 135: 0.0407 - The argument will do nothing to restore investors' faith in the stock market where, as measured by the Bombay stock exchange's 30-share index, shares have fallen 48 per cent from their peak early last year, including a 320 points fall since the government's budget was announced two weeks ago.\n",
      "Sentence ID 74: 0.0401 - But these suggestions were seen in Bombay as a deliberate attempt to deflect attention from the people who in the popular mind are the real suspects - Moslem extremists who might have taken revenge for the riots which hit India after the Ayodhya mosque's destruction and which left 2,000 dead, including 700 in Bombay.\n",
      "Sentence ID 60: 0.0356 - MR TEG BAHADUR Thapar was serving lunch from his stall outside the Bombay stock exchange on Friday when he was showered with flying glass and broken concrete in one of 13 explosions which rocked the city and left at least 255 people killed and over 1,200 injured.\n",
      "Sentence ID 40: 0.0316 - The identity of the bombers was unknown but the explosion comes just five days after over 250 people were killed and at least 1,200 injured in a series of bomb attacks aimed at the heart of the Indian business capital of Bombay.\n",
      "Sentence ID 45: 0.0263 - Last Friday a series of car bombs exploded in Bombay in what the government described as a systematic attack on India's commercial heart.\n",
      "Sentence ID 119: 0.0262 - At the 28-storey stock exchange building, about 3,000 people were on the second-storey trading floor when a bomb exploded in the underground car park, hurling shards of glass across the floor and into the street.\n",
      "Sentence ID 5: 0.0252 - All this has taken place against a volatile background: the Bombay equity market, over the past six weeks or so, has had to contend first with unusually bullish speculation, before the national budget on February 27, and subsequently with some savage expressions of disappointment.\n",
      "Sentence ID 48: 0.0244 - More than 2,000 people died in riots, including over 700 in Bombay, after the razing of a mosque at Ayodhya in northern India by Hindu zealots on December 6.\n",
      "Sentence ID 108: 0.0244 - More than 2,000 people died in riots, including over 700 in Bombay, after the razing of a mosque at Ayodhya in northern India by Hindu zealots on December 6.\n",
      "Sentence ID 38: 0.0221 - INDIA suffered a fresh wave of terror last night when an explosion destroyed two blocks of flats in the centre of the city of Calcutta, killing at least 45 people and injuring scores of others.\n",
      "Sentence ID 28: 0.0206 - The exemption has been replaced by a flat 20 per cent tax this year which, according to dealers, puts bulls and bears on an even footing .\n",
      "Sentence ID 131: 0.0203 - A ROW has erupted between the Bombay stock exchange, India's largest stock market, and the Securities and Exchange Board of India, the securities watchdog, over the board's recent first-ever inspection of stockbrokers' books.\n",
      "Sentence ID 107: 0.0190 - The identity of the bombers was unknown, but the attacks were a severe setback to India's attempts to heal the wounds created by recent intercommunal violence, which has caused deep shock throughout India.\n",
      "Sentence ID 138: 0.0187 - In one of its most damaging passages, the inspectors' report alleges that D S Prabhoodas, a leading firm headed by Mr Hemendra Kothari, a former BSE president, and by Mr Jaswant Chotalal, a member of the BSE's governing body, avoided margin payments totalling Rs185.4m (Pounds 4.08m).\n",
      "Sentence ID 118: 0.0185 - The bomb attacks were spread throughout the city and were clearly aimed at the city's better-off business people - in contrast to the riots in which victims were mainly poor.\n"
     ]
    }
   ],
   "source": [
    "# summary_sentences = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[:int(0.1 * len(pagerank_scores))]\n",
    "# summary = [sentence_dict[i]['sentence_text'] for i in summary_sentences]\n",
    "# print(\"\\nSummary Sentences:\")\n",
    "# for sentence in summary:\n",
    "#     print(sentence)\n",
    "# Get the 10% highest score sentences to create a summary\n",
    "summary_sentences = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[:int(0.1 * len(pagerank_scores))]\n",
    "print(\"Summary Sentences:\")\n",
    "for i in summary_sentences:\n",
    "    print(f\"Sentence ID {i}: {pagerank_scores[i]:.4f} - {sentences_dict[i]['sentence_text']}\")\n",
    "# summary = [sentence_dict[i]['sentence_text'] for i in summary_sentences]\n",
    "# print(\"\\nSummary Sentences:\")\n",
    "# for sentence in summary:\n",
    "#     print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8bf407cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10% sentences written to output/d113h_commonword\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the input file name without the extension\n",
    "input_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_dir, f'{input_filename}_commonword')\n",
    "\n",
    "# Write the top sentences to the output file in the desired format\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for sentence_id in summary_sentences:\n",
    "        doc_id = sentences_dict[sentence_id]['doc_id']\n",
    "        wdcount = sentences_dict[sentence_id]['wdcount']\n",
    "        num = sentences_dict[sentence_id]['num']\n",
    "        sentence_text = sentences_dict[sentence_id]['sentence_text']\n",
    "        # Reconstruct the original sentence tag format\n",
    "        outfile.write(f'<s doc_id=\"{doc_id}\" num=\"{num}\" wdcount=\"{wdcount}\"> {sentence_text}</s>\\n')\n",
    "\n",
    "print(f\"\\nTop 10% sentences written to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2adf71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the result with the summay file has given in DUC_SUM folder\n",
    "\n",
    "preference_sum_path = \"DUC_SUM/d112h\"  \n",
    "with open(preference_sum_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    preference_sum_file = file.read()\n",
    "\n",
    "preference_sum_dict = parse_doc(preference_sum_file)\n",
    "# compare the summary_sentences with the preference summary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7f21aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 11 summary sentences are in the preference summary (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Get the set of (doc_id, num) for sentences in preference_sum_dict\n",
    "preference_keys = set((v['doc_id'], v['num']) for v in preference_sum_dict.values())\n",
    "\n",
    "# Count how many summary_sentences are present in preference_sum_dict by (doc_id, num)\n",
    "matched = 0\n",
    "for sid in summary_sentences:\n",
    "    sent = sentences_dict[sid]\n",
    "    if (sent['doc_id'], sent['num']) in preference_keys:\n",
    "        matched += 1\n",
    "\n",
    "percentage = (matched / len(preference_sum_dict)) * 100 if preference_sum_dict else 0\n",
    "print(f\"{matched} out of {len(preference_sum_dict)} summary sentences are in the preference summary ({percentage:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
